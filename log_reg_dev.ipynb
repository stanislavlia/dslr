{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, lr=1e-4, iterations=1000, l2_penalty=0):\n",
    "        \n",
    "        self.iterations = iterations\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.lr = lr\n",
    "        self.parameters = []\n",
    "\n",
    "    def _init_params(self, n_features):\n",
    "\n",
    "        self.parameters = np.random.normal(size=(n_features + 1), scale=0.1) #initalize n weights + 1 bias randomly\n",
    "        self.parameters = self.parameters.reshape(-1, 1) # make it matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_design_matrix(X):\n",
    "        n, d = X.shape\n",
    "        X_d = np.empty((n, d+1), dtype=X.dtype)\n",
    "        X_d[:, 0] = 1 #column of ones\n",
    "        X_d[:, 1:] = X\n",
    "        return X_d\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(logits):\n",
    "        return 1 / (1 + np.exp(-logits))\n",
    "\n",
    "    def _compute_logits(self, X):        \n",
    "        return (X @ self.parameters)  #(m, n) x (n, 1) =  (m, 1) #logits\n",
    "    \n",
    "    @staticmethod\n",
    "    def _binary_cross_entropy_loss(Y_true, Y_pred):\n",
    "        \n",
    "        loss = - (Y_true * np.log(Y_pred) + (1 - Y_true) * np.log(1 - Y_pred))\n",
    "        return loss.mean()\n",
    "    \n",
    "    def _compute_gradient(self, X_d, Y_true, Y_pred):\n",
    "        \"\"\"\n",
    "        X_d:    (m × (d+1)) design matrix, X_d[:,0] == 1\n",
    "        Y_true: (m × 1) true labels\n",
    "        Y_pred: (m × 1) predicted probabilities\n",
    "        \"\"\"\n",
    "        \n",
    "        m = Y_true.shape[0]\n",
    "        gradient = X_d.T @ (Y_pred - Y_true)  / m  #  (n, m)  @ (m, 1) = (n, 1) #gradient\n",
    "\n",
    "        #l2 regularization\n",
    "        gradient[1:] += (self.parameters[1:] * self.l2_penalty) / m #add sum of weights (except bias term)\n",
    "\n",
    "        return gradient\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        if not (isinstance(X, np.ndarray) and isinstance(Y, (np.ndarray, np.array, list))):\n",
    "            raise ValueError(\"Invalid type for inputs. Expected numpy.ndarray\")\n",
    "        if len(Y.shape) != 2:\n",
    "            raise ValueError(\"Y must be matrix. Hint: do Y.reshape(-1, 1)\")\n",
    "        \n",
    "        self.n_features = X.shape[1]\n",
    "        self._init_params(self.n_features)\n",
    "\n",
    "        #create design matrix\n",
    "        X_d  = self._get_design_matrix(X)\n",
    "\n",
    "        #Training loop\n",
    "        for i in range(self.iterations):\n",
    "            \n",
    "            logits = self._compute_logits(X_d)\n",
    "            Y_pred = self._sigmoid(logits)\n",
    "\n",
    "            #compute loss\n",
    "            loss = self._binary_cross_entropy_loss(Y, Y_pred)\n",
    "            print(f\"ITERATION {i + 1} | BCE LOSS = {float(loss.round(6))}\")\n",
    "\n",
    "            #compute gradient\n",
    "            gradient = self._compute_gradient(X_d, Y, Y_pred)\n",
    "\n",
    "            #update parameters\n",
    "            self.parameters -= self.lr * gradient\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        X_d = self._get_design_matrix(X)\n",
    "        logits = self._compute_logits(X_d)\n",
    "        probs = self._sigmoid(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "\n",
    "\n",
    "class OVALogisticRegression():\n",
    "    \"\"\"One vs All Logistic Regression for Muticlass Classification\"\"\"\n",
    "\n",
    "    def __init__(self, lr=1e-4, iterations=1000, l2_penalty=0):\n",
    "        \n",
    "        self.iterations = iterations\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.lr = lr\n",
    "\n",
    "        self.binary_logregs = []\n",
    "    \n",
    "    def _init_params(self, n_features, num_classes):\n",
    "        \n",
    "        self.binary_logregs = [LogisticRegression(lr=self.lr,\n",
    "                                                  iterations=self.iterations,\n",
    "                                                  l2_penalty=self.l2_penalty) for _ in range(num_classes)]\n",
    "        #init params\n",
    "        for logreg in self.binary_logregs:\n",
    "            logreg._init_params(n_features)\n",
    "    \n",
    "    def dump_parameters(self, filepath):\n",
    "        pass\n",
    "\n",
    "    def load_parameters(self, filepath):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _softmax(logits): \n",
    "        \"\"\"logits - matrix of logits (m, n)\"\"\"\n",
    "\n",
    "        softmax = np.exp(logits) / (np.exp(logits).sum(axis=1)).reshape(-1, 1)\n",
    "        return softmax\n",
    "\n",
    "    @staticmethod\n",
    "    def _onehot_labels(Y):\n",
    "        \"OneHot Encoder for Y\"\n",
    "\n",
    "        n = Y.shape[0]\n",
    "        K = Y.max() + 1\n",
    "        one_hot = np.zeros((n, K), dtype=int)\n",
    "        idx = Y.ravel()\n",
    "        one_hot[np.arange(n), idx] = 1 #set 1 in correct position correspodning to label\n",
    "        return one_hot\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        X_d = LogisticRegression._get_design_matrix(X)\n",
    "\n",
    "        logits = []\n",
    "        for j in range(self.n_classes):\n",
    "            #compute logits for jth class\n",
    "            jth_class_logits = (self.binary_logregs[j]._compute_logits(X_d))\n",
    "            logits.append(jth_class_logits)\n",
    "        #Transpose\n",
    "        logits = np.hstack(logits)  #(m, n_classes)\n",
    "\n",
    "        #get probs\n",
    "        probs = self._softmax(logits)\n",
    "        return probs\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        if not (isinstance(X, np.ndarray) and isinstance(Y, (np.ndarray, np.array, list))):\n",
    "            raise ValueError(\"Invalid type for inputs. Expected numpy.ndarray\")\n",
    "        if len(Y.shape) != 2:\n",
    "            raise ValueError(\"Y must be matrix. Hint: do Y.reshape(-1, 1)\")\n",
    "\n",
    "        self.n_classes = len(np.unique(Y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self._init_params(self.n_features, self.n_classes)\n",
    "\n",
    "        Y_one_hot = self._onehot_labels(Y)\n",
    "        \n",
    "        #train binary logreg for each class separetely\n",
    "        for j, logreg in enumerate(self.binary_logregs):\n",
    "\n",
    "            y_class_labels = Y_one_hot[:, j].reshape(-1, 1) #take jth column from One-hot matrix\n",
    "            logreg.fit(X, y_class_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
